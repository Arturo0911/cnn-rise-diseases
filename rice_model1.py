# -*- coding: utf-8 -*-
"""rice_model1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12sC5TPwGn6Gl7DRjHK4gduPl2KAxNEfR
"""

import tensorflow as tf

from tensorflow.keras import (
    regularizers,
    models,
    optimizers
)
from tensorflow.keras.layers import (
    Dense,
    Conv2D,
    Flatten,
    MaxPooling2D,
    Dropout,
    Activation,
    BatchNormalization
)
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from google.colab import drive


import pickle
import os
import sys
FILE_PATH = "/gdrive/My Drive/NewRiceImages"
drive.mount("/gdrive")

TRAIN_PATH = f"{FILE_PATH}/train"
TEST_PATH = f"{FILE_PATH}/test"

print(os.listdir(FILE_PATH))
print(os.listdir(TRAIN_PATH))
print(os.listdir(TEST_PATH))

# print(f"The numnber of the rice_leaf_helthy are: {os.listdir()}")
print(len(os.listdir(TRAIN_PATH+"/rice_leaf_healthy")))
print(len(os.listdir(TRAIN_PATH+"/rice_leaf_yellow")))
print(len(os.listdir(TEST_PATH+"/rice_leaf_healthy")))
print(len(os.listdir(TEST_PATH+"/rice_leaf_yellow")))

# starting to divide the dataimages

train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    horizontal_flip=True
)

test_gen = ImageDataGenerator(
    rescale = 1./255
)

train_generator = train_gen.flow_from_directory(
    TRAIN_PATH,
    target_size=(64, 64),
    batch_size=32,
    class_mode="categorical"
)
test_generator = test_gen.flow_from_directory(
    TEST_PATH,
    target_size=(64, 64),
    batch_size=32,
    class_mode="categorical"
)

# starting with the model creation

model = Sequential()

# first model convolution
model.add(Conv2D(32, kernel_size=(2,2), padding="same",  kernel_regularizer=regularizers.l2(1e-4), input_shape=(64,64,3)))
model.add(Activation('relu'))

# second model convolution
model.add(Conv2D(32, kernel_size=(2,2), padding="same", kernel_regularizer=regularizers.l2(1e-4)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

# third convolution, more deep
model.add(Conv2D(
    64,
    kernel_size=(2, 2),
    padding="same",
    kernel_regularizer=regularizers.l2(1e-4)
))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(Dropout(0.2))

# fourth convolution
model.add(Conv2D(
    64,
    kernel_size=(2, 2),
    padding="same",
    kernel_regularizer=regularizers.l2(1e-4)
))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.3))

# fifth convolution
model.add(Conv2D(
    64,
    kernel_size=(2, 2),
    padding="same",
    kernel_regularizer=regularizers.l2(1e-4)
))
model.add(Activation("relu"))
model.add(BatchNormalization())

# sixth convolution
model.add(Conv2D(
    64,
    kernel_size=(2, 2),
    padding="same",
    kernel_regularizer=regularizers.l2(1e-4)
))
model.add(Activation("relu"))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.4))

# classification flatten
model.add(Flatten())
model.add(Dense(2, activation="softmax"))

# compiling

model.compile(loss="categorical_crossentropy",
              optimizer=optimizers.Adam(),
              metrics=["accuracy"])

#model.summary()

checkpoint = ModelCheckpoint("/gdrive/My Drive/rice_model.hdf5", monitor="accuracy",
                                     verbose=1, save_best_only=True)

hist = model.fit(train_generator,
                 steps_per_epoch=190//32,
                 verbose=True,
                 callbacks=[checkpoint],
                 epochs=100)

from pprint import pprint
pprint(dir(train_generator))
train_generator.batch_size
test_generator.batch_size

